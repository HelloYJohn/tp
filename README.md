# tidb pingcap
  ## 1. 基本思路
      + 100G的大文件，1G内存肯定无法处理，把大文件hash成小文件（500M左右，具体为什么选，下面会介绍，文件个数大概200左右），不同的url不会分到相同的文件
      + 使用topk算法（最小堆，依据重复的个数） 求每个小文件的前100重复的URL（最后有多个相同的，取最先满足重复个数）放到一个线性容器中（大概2万左右）
      + 对这2万数据进行使用topk算法（依据重复的个数），取前一百即可
      
